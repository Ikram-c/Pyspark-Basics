# Pyspark Basics

## Overview of project

The notebook goes over an introduction to pyspark and covers PysparkSQL, Data wrangling and Pyspark ML. 
This notebook covers PySpark Basics including its architecture, installation, setup and configuration
It then discusses SQL Data Wrangling with PySpark, including reading and writing data from/to various sources, filtering, joining, grouping, and aggregating data
Finally, the notebook discusses machine learning with PySpark, covering topics such as feature engineering, model training, tuning, evaluation, and deployment.
The data used for the notebook was the california housing prices dataset (https://www.kaggle.com/datasets/camnugent/california-housing-prices). 
Data preprocessing is also used to ensure that the ML models had some level of optimisation, the ML model used was Elastic net.
![example of summary statistics pyspark](https://user-images.githubusercontent.com/68299933/215772988-54c650c4-cf2b-41aa-82f9-0f503867e475.jpg)

(sample of summary statistics pyspark)


Note: Some outputs were corrupted due to switching Pyspark servers.
